<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Embedding,Attention,Character,Residual," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Note for Papers in ICLR 2017 about NLPInternational Conference on Learning Representations known as ICLR is one of the topest Conference. Because I have being concentrating on the representation of t">
<meta property="og:type" content="article">
<meta property="og:title" content="ICLR_2017_for_NLP">
<meta property="og:url" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/index.html">
<meta property="og:site_name" content="Study in IRLAB">
<meta property="og:description" content="Note for Papers in ICLR 2017 about NLPInternational Conference on Learning Representations known as ICLR is one of the topest Conference. Because I have being concentrating on the representation of t">
<meta property="og:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/character-aware-attention-residual-network.png">
<meta property="og:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/sentence-representation.png">
<meta property="og:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/attention-mechanism.png">
<meta property="og:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/residual-network.png">
<meta property="og:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/gate-between-word-and-char.png">
<meta property="og:updated_time" content="2017-02-24T03:11:38.366Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ICLR_2017_for_NLP">
<meta name="twitter:description" content="Note for Papers in ICLR 2017 about NLPInternational Conference on Learning Representations known as ICLR is one of the topest Conference. Because I have being concentrating on the representation of t">
<meta name="twitter:image" content="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/character-aware-attention-residual-network.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/"/>





  <title> ICLR_2017_for_NLP | Study in IRLAB </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Study in IRLAB</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">Embedding, RNN and so on</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'Ceo8xts56UsQv4RPjsSy','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="He Guoxiu">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Study in IRLAB">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Study in IRLAB" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                ICLR_2017_for_NLP
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-13T10:28:40+08:00">
                2017-01-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/01/13/ICLR-2017-for-NLP/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/01/13/ICLR-2017-for-NLP/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><meta http-equiv="content-type" content="text/html; charset=UTF-8"></p>
<h1 id="Note-for-Papers-in-ICLR-2017-about-NLP"><a href="#Note-for-Papers-in-ICLR-2017-about-NLP" class="headerlink" title="Note for Papers in ICLR 2017 about NLP"></a>Note for Papers in ICLR 2017 about NLP</h1><p><strong>International Conference on Learning Representations known as ICLR is one of the topest Conference. Because I have being concentrating on the representation of the text more, I have done a review of the papers in ICLR on NLP.</strong><br><a id="more"></a></p>
<h2 id="1-Representation-of-Character-Word-and-Sentence"><a href="#1-Representation-of-Character-Word-and-Sentence" class="headerlink" title="1. Representation of Character, Word and Sentence"></a>1. Representation of Character, Word and Sentence</h2><p><strong>Representations or Embeddings in many ways</strong></p>
<h3 id="1-1-Character-Aware-Attention-Residual-Network-for-Sentence-Representation"><a href="#1-1-Character-Aware-Attention-Residual-Network-for-Sentence-Representation" class="headerlink" title="1.1 Character-Aware Attention Residual Network for Sentence Representation"></a>1.1 Character-Aware Attention Residual Network for Sentence Representation</h3><p><strong>One way about the sentence embedding by Xin Zheng&#xFF08;Nanyang Technological University, Singapore&#xFF09;, Zhenzhou Wu(SAP)</strong></p>
<ul>
<li>Goal: Classify short and noisy text</li>
<li><p>Problem: </p>
<ul>
<li>Feature sparsity using bag-of-word model, with TFIDF or other weighting schemes</li>
<li>Bag-of-word method has an intrinsic disadvantage that two separate features will be generated for two words with the same root or of different tenses. In other word, the morphology is very important to understand the information of the short document.</li>
<li>Word2vec or Doc2Vec which are distributed word representation miss the word morphology information and word combination information.</li>
</ul>
</li>
<li><p>Backgroud: The quality of document representation here has a great impact on the classification accuracy.</p>
</li>
<li><p>Works of this paper:</p>
<ul>
<li>Take word morphology and word semantic meaning into considerationby using character-aware embedding and word distributed embedding.(This may be the common benefit of the embeddings.)</li>
<li>To obtained a sentence representation matrix: concatenate both character-level and word distributed embedding together and arranging words in order. Sentence representation vector is then derived based on different views from sentence representation matrix to overcome data sparsity problem of short text. At last, a residual network is employed on the sentence embedding to get a consistent and refined sentence representation.(The detials will be shown later.)</li>
</ul>
</li>
<li><p>Details of the model:<br>This paper proposes a character-aware attention residual network to generate sentence representation as the Figure shown.<br><img src="/2017/01/13/ICLR-2017-for-NLP/character-aware-attention-residual-network.png" alt="character-aware-attention-residual-network"></p>
<ol>
<li>A matrix constructed by characters embedding in word is encoded into a vector by convolution network.</li>
<li>Concatenate both character-level embedding and word semantic embedding into a word representation vector.</li>
<li>A sentence is represented by a matrix.</li>
<li>Enrich sentence representation vector by Attention Mechanism: solve the problem that not all the features contribute the same for classification(or other tasks) and target on pertinent parts.</li>
<li>Refine sentence representation vector by Residual network: extracte features from different views consistent.</li>
<li>Obtain the final sentence vector for the classification(or other tasks).</li>
</ol>
</li>
<li><p>More details of the model: </p>
<ul>
<li>Word representation construction: $C$ is vocabulary of characters, $E\in R^{d_c\dot |C|}$ is the character embedding matrix, $d_c$ is the dimensionality of character embedding, $E^w\in R^{d_c\dot n_c}$ is word character-level embedding matrix, $E_i^w=E\dot v_i$ where $v_i$ is a binary column vector that is one row in $E$. Then use convolution network to get the vector $q_c$ which captures the character-level information.(But I still don&#x2019;t know how does the paper solve the problem that the dimentions of the matrix are not same about all words. Maybe some skills known as the padding?) The character-level embedding can only caputure the word morphological features, therefore concatenating the distributed word representative vector as the reflect of the word semantic and syntactic characteristics.</li>
<li>Sentence representation vector construction: shown as below:<br><img src="/2017/01/13/ICLR-2017-for-NLP/sentence-representation.png" alt="sentence-representation"><br>Using different weights for every vector of matirx and attention mechanism to enrich the sentence representation.<ul>
<li>attention mechanism shown as below.<br><img src="/2017/01/13/ICLR-2017-for-NLP/attention-mechanism.png" alt="attention-mechanism"><br>$g(q_i)=Tanh(W_{qh}q_i+b_{qh})$<br>$s_i=\frac{exp(g(q_i))}{\sum_n_w^{j=i}exp(g(q_j))}, \hat q_i=s_iq_i$</li>
<li>convolution operations on $Q$ with n-grams.</li>
</ul>
</li>
<li>Residual Network for Refining Sentence Representation: shown as below:<br><img src="/2017/01/13/ICLR-2017-for-NLP/residual-network.png" alt="residual-network"><br>That is one kind of convolution network. But I konw nothing about the residual.-_-|| So let it go.:)</li>
</ul>
</li>
<li><p>Experiments: The model outperforms stat-of-the-art models on a few short-text datasets.</p>
<ul>
<li>Dataset<br>|Dataset|classes|Train Samples|Test Samples|Average Length of text|<br>|:&#x2014;&#x2013;:|:&#x2014;&#x2013;:|:&#x2014;&#x2014;&#x2014;&#x2013;:|:&#x2014;&#x2014;&#x2014;-:|:&#x2014;&#x2014;&#x2014;&#x2014;&#x2014;&#x2014;&#x2013;:|<br>|Tweet|5|28,000|7,500|7|<br>|Question|5|2,000|700|25|<br>|AG_news|5|120,000|7,600|20|</li>
<li>Other details of the experiment is ignored by me.:)</li>
<li>The result is very good.:)</li>
</ul>
</li>
<li><p>High insight:</p>
<blockquote>
<ul>
<li>We must explain the word-level representation about the Chinese. And that is important also.</li>
<li>Attention mechanism which focuses on specific part of input could help achieve this goal that not all the words in a sentence contribute the same when predicting the sentence&#x2019;s label</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="1-2-Program-Synthesis-for-Character-Level-Language-Modeling"><a href="#1-2-Program-Synthesis-for-Character-Level-Language-Modeling" class="headerlink" title="1.2 Program Synthesis for Character Level Language Modeling"></a>1.2 Program Synthesis for Character Level Language Modeling</h3><p><strong>a character level language modeling created by Pavol Bielik, Veselin Raychev &amp; Martin Vechev in Department of Computer Science</strong></p>
<ul>
<li>Goal: a character level language modeling for both program source code and English text.</li>
<li>How to do: the model is parameterized by a program from a domain-specific language(DSL).</li>
</ul>
<h3 id="1-3-Words-or-Characters-Fine-Grained-Gating-for-Reading-Comprehension"><a href="#1-3-Words-or-Characters-Fine-Grained-Gating-for-Reading-Comprehension" class="headerlink" title="1.3 Words or Characters? Fine-Grained Gating for Reading Comprehension"></a>1.3 Words or Characters? Fine-Grained Gating for Reading Comprehension</h3><p><strong>A mode for reading comprehension created by Zhilin Yang, Bhuwan Dhingra, Ye Yuan, Junjie Hu, William W. Cohen, Ruslan Salakhutdinov in CMU</strong></p>
<ul>
<li><strong>This paper should be read more carefully!</strong></li>
<li>Goal: The authors present a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on properties of the word and model the interaction between questions and paragraphs for reading comprehension.</li>
<li>Method: The authors compute a vector gate as a linear projection of the token features followed by a sigmoid avtivation. Then multiplicatively apply the gate to the character-level and word-level representations. Each dimension of the gate controls how much information is flowed from the word-level and character-level representations respectively. The gate is determined by named entity tags, part-of-speech tags, document frequencies, and word-level representations as the features for token properties. The gating mechanism can be generally used to model multiple levels of structure in language, including words, characters, phrases, sentences and paragraphs.<br><img src="/2017/01/13/ICLR-2017-for-NLP/gate-between-word-and-char.png" alt="gate-between-word-and-char"></li>
<li>Datasets: Children&#x2019;s Book Test dataset.</li>
<li>Tasks: children&#x2019;s book test dataset and social media tag prediction task.</li>
<li>Experiments: Their approach can improve the performance on reading comprehension task.</li>
<li>advantage: Character-level representations are used to alleviate the difficulties of modeling out-of-vocabulary(OOV) tokens.</li>
</ul>
<h3 id="1-4-Deep-Character-Level-Neural-Machine-Translation-By-Learning-Morphology"><a href="#1-4-Deep-Character-Level-Neural-Machine-Translation-By-Learning-Morphology" class="headerlink" title="1.4 Deep Character-Level Neural Machine Translation By Learning Morphology"></a>1.4 Deep Character-Level Neural Machine Translation By Learning Morphology</h3><p><strong>A new method for NMT by Shenjian Zhao in Shanghai Jiao Tong University and Zhihua Zhang in Peking University</strong></p>
<ul>
<li>Goal: NMT aims at building a single large neural network that can be trained to maximize translation preformance.</li>
<li>Problem: The use of large vocabulary becomes the bottleneck in both training and improving the performance.</li>
<li>Methods: Two recurrent networks and a hierarchical decoder which translates at character level.</li>
<li>Advantages: It avoids the large vocabulary issue radically; It is more efficient in training than word-based models.</li>
<li>Experiments: Higer BLEU and learn more morphology.</li>
</ul>
<h3 id="1-5-Opening-The-Vocabulary-of-Neural-Language-Models-with-Character-Level-Word-Representations"><a href="#1-5-Opening-The-Vocabulary-of-Neural-Language-Models-with-Character-Level-Word-Representations" class="headerlink" title="1.5 Opening The Vocabulary of Neural Language Models with Character-Level Word Representations"></a>1.5 Opening The Vocabulary of Neural Language Models with Character-Level Word Representations</h3><p><strong>a opening-vocabulary neural language model by Matthieu Labeau and Alexandre Allauzen in LIMSI-CNRS / Orsay, France</strong></p>
<ul>
<li>Goal: an open-vocabulary neural language model</li>
<li>Advantage: can consider any word, that is the model can build representations of unknown words.</li>
<li>Experiment: gain up tp 0.7 BLEU point.</li>
</ul>
<h3 id="1-6-Unsupervised-Sentence-Representation-Learning-With-Adversarial-Auto-Encoder"><a href="#1-6-Unsupervised-Sentence-Representation-Learning-With-Adversarial-Auto-Encoder" class="headerlink" title="1.6 Unsupervised Sentence Representation Learning With Adversarial Auto-Encoder"></a>1.6 Unsupervised Sentence Representation Learning With Adversarial Auto-Encoder</h3><p><strong>A fixed dimension to represent sentence by Shuai Tang(UC San Diego) and Hailin Jin &amp; Chen Fang &amp; Zhaowen Wang(Adobe Research)</strong></p>
<ul>
<li>Goal: a fixed dimension</li>
<li>Challenge: capture both of the semantic and structural information conveyed by a sentence.</li>
<li>Advantage: learn representation from the unlabeled large corpus text data.</li>
</ul>
<h3 id="1-7-Offline-Bilingual-Word-Vectors-Without-A-Dictionary"><a href="#1-7-Offline-Bilingual-Word-Vectors-Without-A-Dictionary" class="headerlink" title="1.7 Offline Bilingual Word Vectors Without A Dictionary"></a>1.7 Offline Bilingual Word Vectors Without A Dictionary</h3><p><strong>Word Vectors for Bilingual as offline by Samuel L. Smith, David H. P. Turban, Nils Y. Hammerla &amp; Steven Hamblin in London, SW3 3DD, UK</strong></p>
<ul>
<li>Goal: A model that two pre-trained embeddings are aligned by a linear transformation, using dictionaries compiled from expert knowledge.</li>
<li>Method: &#x201C;inverted softmax&#x201D; for identifying translation pairs.</li>
</ul>
<h3 id="1-8-Learning-Word-like-Units-from-Joint-Audio-Visual-Analysis"><a href="#1-8-Learning-Word-like-Units-from-Joint-Audio-Visual-Analysis" class="headerlink" title="1.8 Learning Word-like Units from Joint Audio-Visual Analysis"></a>1.8 Learning Word-like Units from Joint Audio-Visual Analysis</h3><p><strong>created by David Harwath and James R. Glass in Massachusetts Institute of Technology</strong></p>
<ul>
<li>Goal: A method for discorvering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions from given a collection of images and spoken audio captions.</li>
</ul>
<h3 id="1-9-Tying-Word-Vectors-and-Word-Classifiers-A-Loss-Framework-for-Language-Modeling"><a href="#1-9-Tying-Word-Vectors-and-Word-Classifiers-A-Loss-Framework-for-Language-Modeling" class="headerlink" title="1.9 Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"></a>1.9 Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling</h3><p><strong>A new framework created by Massachusetts Institute of Technology(in Stanford University) and Richard Socher(in Salesforce Research)</strong></p>
<ul>
<li>Goal: A new framework</li>
<li>Advantage: greatly reducing the number of trainable variables.</li>
<li>Experiments: Their LSTM model lowers the state of the art word-level perplexity on the Penn Treebank to 68.5.</li>
</ul>
<h3 id="1-10-Sentence-Ordering-Using-Recurrent-Neural-Networks"><a href="#1-10-Sentence-Ordering-Using-Recurrent-Neural-Networks" class="headerlink" title="1.10 Sentence Ordering Using Recurrent Neural Networks"></a>1.10 Sentence Ordering Using Recurrent Neural Networks</h3><p><strong>A model for structure of coherent text created by Lajanugen Logeswaran, Honglak Lee &amp; Dragomir Radev in University of Michigan</strong></p>
<ul>
<li>Goal: an end-to-end neural approach based on the proposed set to sequence mapping framework to address the sentence ordering problem.</li>
<li>Method: &#x2026;</li>
<li>Result: the model has captured high level logical structure in these paragraphs and also learns rich semantic sentence representations.</li>
</ul>
<h2 id="2-Retrieval-Q-amp-A-Recommend-System"><a href="#2-Retrieval-Q-amp-A-Recommend-System" class="headerlink" title="2. Retrieval / Q&amp;A / Recommend System"></a>2. Retrieval / Q&amp;A / Recommend System</h2><p><strong>some applications by using NN</strong></p>
<h3 id="2-1-Learning-to-Query-Reason-and-Answer-Questions-on-Ambiguous-Texts"><a href="#2-1-Learning-to-Query-Reason-and-Answer-Questions-on-Ambiguous-Texts" class="headerlink" title="2.1 Learning to Query, Reason, and Answer Questions on Ambiguous Texts"></a>2.1 Learning to Query, Reason, and Answer Questions on Ambiguous Texts</h3><h3 id="2-2-Group-Sparse-CNNs-for-Question-Sentence-Classification-with-Answer-Sets"><a href="#2-2-Group-Sparse-CNNs-for-Question-Sentence-Classification-with-Answer-Sets" class="headerlink" title="2.2 Group Sparse CNNs for Question Sentence Classification with Answer Sets"></a>2.2 Group Sparse CNNs for Question Sentence Classification with Answer Sets</h3><h3 id="2-3-Content2Vec-Specializing-Joint-Representations-of-Product-Images-and-Text-for-the-task-Product-Recommendation"><a href="#2-3-Content2Vec-Specializing-Joint-Representations-of-Product-Images-and-Text-for-the-task-Product-Recommendation" class="headerlink" title="2.3 Content2Vec: Specializing Joint Representations of Product Images and Text for the task Product Recommendation"></a>2.3 Content2Vec: Specializing Joint Representations of Product Images and Text for the task Product Recommendation</h3><h3 id="2-4-Is-a-picture-worth-a-thousand-words-A-Deep-Multi-Modal-Fusion-Architecture-for-Product-Classification-in-e-commerce"><a href="#2-4-Is-a-picture-worth-a-thousand-words-A-Deep-Multi-Modal-Fusion-Architecture-for-Product-Classification-in-e-commerce" class="headerlink" title="2.4 Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce"></a>2.4 Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce</h3><h2 id="3-Word-Sentence-Embedding"><a href="#3-Word-Sentence-Embedding" class="headerlink" title="3. Word/Sentence Embedding"></a>3. Word/Sentence Embedding</h2><p><strong>some ways for embedding</strong></p>
<h3 id="3-1-A-Simple-but-Tough-to-Beat-Baseline-for-Sentence-Embeddings"><a href="#3-1-A-Simple-but-Tough-to-Beat-Baseline-for-Sentence-Embeddings" class="headerlink" title="3.1 A Simple but Tough-to-Beat Baseline for Sentence Embeddings"></a>3.1 A Simple but Tough-to-Beat Baseline for Sentence Embeddings</h3><h3 id="3-2-Investigating-Different-Context-Types-and-Representations-for-Learning-Word-Embedding"><a href="#3-2-Investigating-Different-Context-Types-and-Representations-for-Learning-Word-Embedding" class="headerlink" title="3.2 Investigating Different Context Types and Representations for Learning Word Embedding"></a>3.2 Investigating Different Context Types and Representations for Learning Word Embedding</h3><h3 id="3-3-Multi-view-Recurrent-Neural-Acoustic-Word-Embeddings"><a href="#3-3-Multi-view-Recurrent-Neural-Acoustic-Word-Embeddings" class="headerlink" title="3.3 Multi-view Recurrent Neural Acoustic Word Embeddings"></a>3.3 Multi-view Recurrent Neural Acoustic Word Embeddings</h3><h3 id="3-4-A-Self-Attentive-Sentence-Embedding"><a href="#3-4-A-Self-Attentive-Sentence-Embedding" class="headerlink" title="3.4 A Self-Attentive Sentence Embedding"></a>3.4 A Self-Attentive Sentence Embedding</h3><h3 id="3-5-Fine-grained-Analysis-of-Sentence-Embeddings-Using-Auxiliary-Prediction-Tasks"><a href="#3-5-Fine-grained-Analysis-of-Sentence-Embeddings-Using-Auxiliary-Prediction-Tasks" class="headerlink" title="3.5 Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"></a>3.5 Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</h3>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Embedding/" rel="tag"># Embedding</a>
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
            <a href="/tags/Character/" rel="tag"># Character</a>
          
            <a href="/tags/Residual/" rel="tag"># Residual</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/01/08/Learning-Phrase-Representations-using-RNN-Encoder-Decoder-for-Statistical-Machine-Traslation/" rel="next" title="Learning Phrase Representations<br>using RNN Encoder Decoder for Statistical Machine Traslation">
                <i class="fa fa-chevron-left"></i> Learning Phrase Representations<br>using RNN Encoder Decoder for Statistical Machine Traslation
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/01/16/Introduction-to-TkPro-BrokenDetection/" rel="prev" title="Introduction to BrokenDetection">
                Introduction to BrokenDetection <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/01/13/ICLR-2017-for-NLP/"
           data-title="ICLR_2017_for_NLP" data-url="https://frankblood.github.io/2017/01/13/ICLR-2017-for-NLP/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="He Guoxiu" />
          <p class="site-author-name" itemprop="name">He Guoxiu</p>
          <p class="site-description motion-element" itemprop="description">Some notes for interesting papers, tutorials for useful tools, and inspire for life.</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/FrankBlood" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Note-for-Papers-in-ICLR-2017-about-NLP"><span class="nav-number">1.</span> <span class="nav-text">Note for Papers in ICLR 2017 about NLP</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Representation-of-Character-Word-and-Sentence"><span class="nav-number">1.1.</span> <span class="nav-text">1. Representation of Character, Word and Sentence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Character-Aware-Attention-Residual-Network-for-Sentence-Representation"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 Character-Aware Attention Residual Network for Sentence Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Program-Synthesis-for-Character-Level-Language-Modeling"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 Program Synthesis for Character Level Language Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Words-or-Characters-Fine-Grained-Gating-for-Reading-Comprehension"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 Words or Characters? Fine-Grained Gating for Reading Comprehension</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Deep-Character-Level-Neural-Machine-Translation-By-Learning-Morphology"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 Deep Character-Level Neural Machine Translation By Learning Morphology</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Opening-The-Vocabulary-of-Neural-Language-Models-with-Character-Level-Word-Representations"><span class="nav-number">1.1.5.</span> <span class="nav-text">1.5 Opening The Vocabulary of Neural Language Models with Character-Level Word Representations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-Unsupervised-Sentence-Representation-Learning-With-Adversarial-Auto-Encoder"><span class="nav-number">1.1.6.</span> <span class="nav-text">1.6 Unsupervised Sentence Representation Learning With Adversarial Auto-Encoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-Offline-Bilingual-Word-Vectors-Without-A-Dictionary"><span class="nav-number">1.1.7.</span> <span class="nav-text">1.7 Offline Bilingual Word Vectors Without A Dictionary</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-Learning-Word-like-Units-from-Joint-Audio-Visual-Analysis"><span class="nav-number">1.1.8.</span> <span class="nav-text">1.8 Learning Word-like Units from Joint Audio-Visual Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-Tying-Word-Vectors-and-Word-Classifiers-A-Loss-Framework-for-Language-Modeling"><span class="nav-number">1.1.9.</span> <span class="nav-text">1.9 Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-Sentence-Ordering-Using-Recurrent-Neural-Networks"><span class="nav-number">1.1.10.</span> <span class="nav-text">1.10 Sentence Ordering Using Recurrent Neural Networks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Retrieval-Q-amp-A-Recommend-System"><span class="nav-number">1.2.</span> <span class="nav-text">2. Retrieval / Q&A / Recommend System</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Learning-to-Query-Reason-and-Answer-Questions-on-Ambiguous-Texts"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 Learning to Query, Reason, and Answer Questions on Ambiguous Texts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Group-Sparse-CNNs-for-Question-Sentence-Classification-with-Answer-Sets"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 Group Sparse CNNs for Question Sentence Classification with Answer Sets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Content2Vec-Specializing-Joint-Representations-of-Product-Images-and-Text-for-the-task-Product-Recommendation"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 Content2Vec: Specializing Joint Representations of Product Images and Text for the task Product Recommendation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Is-a-picture-worth-a-thousand-words-A-Deep-Multi-Modal-Fusion-Architecture-for-Product-Classification-in-e-commerce"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Word-Sentence-Embedding"><span class="nav-number">1.3.</span> <span class="nav-text">3. Word/Sentence Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-A-Simple-but-Tough-to-Beat-Baseline-for-Sentence-Embeddings"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 A Simple but Tough-to-Beat Baseline for Sentence Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Investigating-Different-Context-Types-and-Representations-for-Learning-Word-Embedding"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 Investigating Different Context Types and Representations for Learning Word Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Multi-view-Recurrent-Neural-Acoustic-Word-Embeddings"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 Multi-view Recurrent Neural Acoustic Word Embeddings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-A-Self-Attentive-Sentence-Embedding"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4 A Self-Attentive Sentence Embedding</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Fine-grained-Analysis-of-Sentence-Embeddings-Using-Auxiliary-Prediction-Tasks"><span class="nav-number">1.3.5.</span> <span class="nav-text">3.5 Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">He Guoxiu</span>
</div>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"frankblood"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  










  
  

  

  

  

  


</body>
</html>
