<!doctype html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Attention,ACL,NLSM," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="A Survey of AttentionSome papers about Attention in NLSM">
<meta property="og:type" content="article">
<meta property="og:title" content="A_Survey_of_Attention">
<meta property="og:url" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/index.html">
<meta property="og:site_name" content="Study in IRLAB">
<meta property="og:description" content="A Survey of AttentionSome papers about Attention in NLSM">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/attentionNMT.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/OverviewOfBiMPM.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/FullMatching.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/MaxpoolingMatching.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/AttentiveMatching.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/MaxAttentiveMatching.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonParaphraseIdentification.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonNaturalLanguageInference.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonAnswerSentenceSelection.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/TraditionalAttentionBasedRNN.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IARNNWORD.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IARNNCONTEXT.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IABRNNGATE.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/OARNN2.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IARNNDataset.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IARNNInsuranceQA.png">
<meta property="og:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/IARNNTrecQA.png">
<meta property="og:updated_time" content="2017-09-29T15:09:24.996Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A_Survey_of_Attention">
<meta name="twitter:description" content="A Survey of AttentionSome papers about Attention in NLSM">
<meta name="twitter:image" content="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/attentionNMT.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/"/>





  <title> A_Survey_of_Attention | Study in IRLAB </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Study in IRLAB</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle">Embedding, RNN and so on</p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="st-search-show-outputs">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <form class="site-search-form">
  <input type="text" id="st-search-input" class="st-search-input st-default-search-input" />
</form>

<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install', 'Ceo8xts56UsQv4RPjsSy','2.0.0');
</script>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://frankblood.github.io/2017/06/19/A-Survey-of-Attention/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="He Guoxiu">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="/images/avatar.png">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Study in IRLAB">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Study in IRLAB" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                A_Survey_of_Attention
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-19T15:06:25+08:00">
                2017-06-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><meta http-equiv="content-type" content="text/html; charset=UTF-8"></p>
<h1 id="A-Survey-of-Attention"><a href="#A-Survey-of-Attention" class="headerlink" title="A Survey of Attention"></a>A Survey of Attention</h1><p><strong>Some papers about Attention in NLSM</strong><br><a id="more"></a></p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="What-and-Why"><a href="#What-and-Why" class="headerlink" title="What and Why"></a>What and Why</h3><p>Reference: <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="external">http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/</a></p>
<ul>
<li><p>What is Attention Mechanisms</p>
<blockquote>
<p>Attention Mechanisms in Neural Networks are (very) loosely based on the visual attention mechanism found in humans. There exist different models come down to being able to focus on a certain region of an image with &#x201C;high resolution&#x201D; while perceiving the surrounding image in &#x201C;low resolution&#x201D;, and then adjusting the focal point over time.</p>
</blockquote>
</li>
<li><p>Why we need Attention Mechanisms</p>
<blockquote>
<p>It seems somewhat unreasonable to assume that RNN can encode all information about a potentially very long sentence into a single vector. </p>
</blockquote>
</li>
<li><p>How does Attention Mechanisms work</p>
<blockquote>
<p>With an attention mechanism we no longer try encode the full source sentence into a fixed-length vector. Rather, we allow the decoder to &#x201C;attend&#x201D; to different parts of the source sentence at each step of the output generation.</p>
</blockquote>
</li>
</ul>
<h3 id="show"><a href="#show" class="headerlink" title="show"></a>show</h3><p><img src="/2017/06/19/A-Survey-of-Attention/attentionNMT.png" alt="attentionNMT"></p>
<p>The important part is that each decoder output word $y<em>t$ now depends on a weighted combination(sum to 1) of all the input states, not just the last state. The $a$&#x2019;s are weights that define in how much of each input state should be considered for each output. So, if $a</em>{3,2}$ is a large number, this would mean that the decoder pays a lot of attention to the second state in the source sentence while producing the third word of the target sentence.</p>
<h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><h2 id="Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences"><a href="#Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences" class="headerlink" title="Bilateral Multi-Perspective Matching for Natural Language Sentences"></a>Bilateral Multi-Perspective Matching for Natural Language Sentences</h2><p><strong>ZhiguoWang,Wael Hamza, Radu Florian IBM T.J.Watson Research Center arXiv 2017</strong></p>
<h3 id="Points"><a href="#Points" class="headerlink" title="Points"></a>Points</h3><ul>
<li>Natural language sentence matching is a fundamental technology for a variety of tasks.</li>
<li>Given two sentences P and Q, our model first encodes them with a BiLSTM encoder. </li>
<li>We match the two encoded sentences in two directions P&#x2192;Q and P&#x2190;Q. In each matching direction, each time step of one sentence is matched against all time-steps of the other sentence from multiple perspectives. Then, another BiLSTM layer is utilized to aggregate the matching results into a fix-length matching vector. </li>
<li>Based on the matching vector, the decision is made through a fully connected layer.</li>
<li>We evaluate our model on three tasks: paraphrase identification, natural language inference and answer sentence selection(achieves the state-of-the-art performance on all tasks).</li>
</ul>
<h3 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h3><ul>
<li>Overview<br><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/OverviewOfBiMPM.png" alt="OverviewOfBiMPM"></li>
</ul>
<p>$\otimes$ is multi-perspective matching operation. $m=f_m(v_1,v_2;W)$, where $v_1$ and $v_2$ are two $d$ dimensional vectors, $W\in R^{l\times d}$ is a trainable parameter, $l$ is the number of perspectives, and the returned $m$ is a $l$ dimensional vector. $m_k=cosine(W_k\circ v_1, W_k\circ v_2)$</p>
<ul>
<li>Full Matching<br><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/FullMatching.png" alt="FullMatching"></li>
</ul>
<p>In this strategy, each forward (or backward) contextual embedding $h_i^p$ is compared with the last time step of the forward (or backward)representation of the other sentence $h_N^q$. $m_i^{full}=f_m(h_i^p, h_N^q; W)$</p>
<ul>
<li>Maxpooling Matching<br><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/MaxpoolingMatching.png" alt="MaxpoolingMatching"></li>
</ul>
<p>In this strategy, each forward (or backward) contextual embedding $h_i^p$ is compared with every forward contextual embeddings of the other sentence $h_j^q$ for $j \in (1&#x2026;N)$, and only the maximum value of each dimension is retained. $m<em>i^{max}=max</em>{j\in (1\cdots N)} f_m(h_i^p, h_j^q; W)$</p>
<ul>
<li>Attentive Matching<br><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/AttentiveMatching.png" alt="AttentiveMatching"></li>
</ul>
<p>First calculate the cosine similarities between each forward contextual embedding $h_i^p$ and every forward contextual embeddings of the other sentence $h<em>j^q$: $\alpha</em>{i,j}=consine(h_i^p, h_j^q), j=1, \cdots, N$</p>
<p>Then, take $\alpha_{i,j}$ as the weight of $h_j^q$ and calculate an attentive vector for the entire sentence Q by weighted summing all the contextual embeddings of Q: $h<em>i^{mean}=\frac{\sum</em>{j=1}^{N}\alpha_{i,j}\dot h<em>j^q}{\sum</em>{j=1}^N\alpha_{i,j}}$</p>
<p>Finally, we match each forward contextual embedding of $h_i^p$ with its corresponding attentive vector: $m_i^{att}=f_m(h_i^p, h_i^{mean}; W)$</p>
<ul>
<li>Max Attentive Matching<br><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/MaxAttentiveMatching.png" alt="MaxAttentiveMatching"></li>
</ul>
<p>Instead of taking the weighed sum of all the contextual embeddings as the attentive vector, they pick \alert{the contextual embedding with the highest cosine similarity} as the attentive vector. Then, we match each contextual embedding of the sentence P with its new attentive vector.</p>
<h3 id="Experiments-on-Paraphrase-Identification"><a href="#Experiments-on-Paraphrase-Identification" class="headerlink" title="Experiments on Paraphrase Identification"></a>Experiments on Paraphrase Identification</h3><p><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonParaphraseIdentification.png" alt="ExperimentsonParaphraseIdentification"></p>
<p><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonNaturalLanguageInference.png" alt="ExperimentsonNaturalLanguageInference"></p>
<p><img src="/2017/06/19/A-Survey-of-Attention/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/ExperimentsonAnswerSentenceSelection.png" alt="ExperimentsonAnswerSentenceSelection"></p>
<h2 id="Inner-Attention-based-Recurrent-Neural-Networks-for-Answer-Selection"><a href="#Inner-Attention-based-Recurrent-Neural-Networks-for-Answer-Selection" class="headerlink" title="Inner Attention based Recurrent Neural Networks for Answer Selection"></a>Inner Attention based Recurrent Neural Networks for Answer Selection</h2><p><strong>BingningWang, Kang Liu, Jun Zhao Institute of Automation Chinese Academy of Sciences ACL 2016</strong></p>
<h3 id="Points-1"><a href="#Points-1" class="headerlink" title="Points"></a>Points</h3><ul>
<li>Attention based recurrent neural networks in representing natural language sentences.</li>
<li>Analyze the deficiency of traditional attention based RNN models quantitatively and qualitatively. </li>
<li>Present three new RNN models that add attention information before RNN hidden representation.</li>
<li>Experimental results show advantage in repre- senting sentence and achieves new state-of-art results in answer selection task.</li>
</ul>
<h3 id="Traditional-Attention-based-RNN-Models"><a href="#Traditional-Attention-based-RNN-Models" class="headerlink" title="Traditional Attention based RNN Models"></a>Traditional Attention based RNN Models</h3><p><img src="/2017/06/19/A-Survey-of-Attention/TraditionalAttentionBasedRNN.png" alt="TraditionalAttentionBasedRNN"></p>
<p>$H_a=[h_a(1),h_a(2),\cdots, h_a(m)]$</p>
<p>$m(t)=tanh(W_{hm}h<em>a(t)+W</em>{qm}r_q)$</p>
<p>$f_{attention}(r_q, h<em>a(t))=exp(w</em>{ms}^Tm(t))$</p>
<p>$s<em>t\propto f</em>{attention}(r_q, h_a(t))$</p>
<p>$\tilde{h}_a(t)=h_a(t)s_t$</p>
<p>$r<em>a=\sum</em>{t=1}^m\tilde{h}_a(t)$</p>
<p>answer sentence representation $r_a$ may be represented in a question-guided way: when its hidden state $h_a(t)$ is irrelevant to the question (determined by attention weight $s_t$), it will take less part in the final representation; but when this hidden state is relavent to the question, it will contribute more in representing $r_a$.</p>
<h3 id="IARNN-WORD"><a href="#IARNN-WORD" class="headerlink" title="IARNN WORD"></a>IARNN WORD</h3><p><img src="/2017/06/19/A-Survey-of-Attention/IARNNWORD.png" alt="IARNNWORD"></p>
<p>$\alpha_t=\sigma(r<em>q^TM</em>{qi}x_t)$</p>
<p>$\tilde{x}_t=\alpha_t*x_t$</p>
<p>$M_{qi}$ is an attention matrix to transform a question representaion into the word embedding space. Then we use the dot value to determine the question attention strength, $\theta$ is sigmoid function to normalize the weight $\alpha_t$ between 0 and 1.</p>
<h3 id="IARNN-CONTEXT"><a href="#IARNN-CONTEXT" class="headerlink" title="IARNN CONTEXT"></a>IARNN CONTEXT</h3><p><img src="/2017/06/19/A-Survey-of-Attention/IARNNCONTEXT.png" alt="IARNNCONTEXT"></p>
<p>$w<em>C(t)=M</em>{hc}h<em>{t-1}+M</em>{qc}r_q$</p>
<p>$\alpha_C^t=\sigma(w_C^T(t)x_t)$</p>
<p>$\tilde{x}_t=\alpha_C^t\ast x_t$</p>
<p>Use $h<em>{t&#x2212;1}$ as context, $M</em>{hc}$ and $M_{qc}$ are attention weight matrices, $w_C(t)$ is the attention representation which consists of both question and word context information.</p>
<h3 id="IARNN-GATE"><a href="#IARNN-GATE" class="headerlink" title="IARNN GATE"></a>IARNN GATE</h3><p><img src="/2017/06/19/A-Survey-of-Attention/IABRNNGATE.png" alt="IABRNNGATE"></p>
<p>$z<em>t=\sigma(W</em>{xz}x<em>t+W</em>{hz}h<em>{h-t}+M</em>{qz}r_q)$</p>
<p>$f<em>t=\sigma(W</em>{xf}x<em>t+W</em>{hf}h<em>{t-1}+M</em>{qf}r_q)$</p>
<p>$\tilde{h}<em>t=tanh(W</em>{xh}x<em>t+W</em>{hh}(f<em>t\odot h</em>{t-1}))$</p>
<p>$h_t=(1-z<em>t)\odot h</em>{t-1}+z_t\odot \tilde{h}_t$</p>
<p>$M<em>{qz}$ and $M</em>{hz}$ are attention weight matrices. In thisway, the update and forget units in GRU can focus on not only long and short term memory but also the attention information from the question.</p>
<h3 id="IARNN-OCCAM"><a href="#IARNN-OCCAM" class="headerlink" title="IARNN OCCAM"></a>IARNN OCCAM</h3><p>I don&#x2019;t know what is it.</p>
<h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><img src="/2017/06/19/A-Survey-of-Attention/OARNN2.png" alt="OARNN2"></p>
<p><img src="/2017/06/19/A-Survey-of-Attention/IARNNDataset.png" alt="IARNNDataset"></p>
<p><img src="/2017/06/19/A-Survey-of-Attention/IARNNInsuranceQA.png" alt="IARNNInsuranceQA"></p>
<p><img src="/2017/06/19/A-Survey-of-Attention/IARNNTrecQA.png" alt="IARNNTrecQA"></p>
<h1 id="To-be-continued&#x2026;"><a href="#To-be-continued&#x2026;" class="headerlink" title="To be continued&#x2026;"></a>To be continued&#x2026;</h1>
      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Attention/" rel="tag"># Attention</a>
          
            <a href="/tags/ACL/" rel="tag"># ACL</a>
          
            <a href="/tags/NLSM/" rel="tag"># NLSM</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/14/ACL-2017-Overview/" rel="next" title="ACL 2017 Overview">
                <i class="fa fa-chevron-left"></i> ACL 2017 Overview
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/09/29/Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences/" rel="prev" title="Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences">
                Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="He Guoxiu" />
          <p class="site-author-name" itemprop="name">He Guoxiu</p>
          <p class="site-description motion-element" itemprop="description">Some notes for interesting papers, tutorials for useful tools, and inspire for life.</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">35</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/FrankBlood" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Github
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#A-Survey-of-Attention"><span class="nav-number">1.</span> <span class="nav-text">A Survey of Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-and-Why"><span class="nav-number">1.1.1.</span> <span class="nav-text">What and Why</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#show"><span class="nav-number">1.1.2.</span> <span class="nav-text">show</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Papers"><span class="nav-number">2.</span> <span class="nav-text">Papers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences"><span class="nav-number">2.1.</span> <span class="nav-text">Bilateral Multi-Perspective Matching for Natural Language Sentences</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Points"><span class="nav-number">2.1.1.</span> <span class="nav-text">Points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Models"><span class="nav-number">2.1.2.</span> <span class="nav-text">Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiments-on-Paraphrase-Identification"><span class="nav-number">2.1.3.</span> <span class="nav-text">Experiments on Paraphrase Identification</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inner-Attention-based-Recurrent-Neural-Networks-for-Answer-Selection"><span class="nav-number">2.2.</span> <span class="nav-text">Inner Attention based Recurrent Neural Networks for Answer Selection</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Points-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">Points</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Traditional-Attention-based-RNN-Models"><span class="nav-number">2.2.2.</span> <span class="nav-text">Traditional Attention based RNN Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IARNN-WORD"><span class="nav-number">2.2.3.</span> <span class="nav-text">IARNN WORD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IARNN-CONTEXT"><span class="nav-number">2.2.4.</span> <span class="nav-text">IARNN CONTEXT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IARNN-GATE"><span class="nav-number">2.2.5.</span> <span class="nav-text">IARNN GATE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IARNN-OCCAM"><span class="nav-number">2.2.6.</span> <span class="nav-text">IARNN OCCAM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Experiments"><span class="nav-number">2.2.7.</span> <span class="nav-text">Experiments</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#To-be-continued…"><span class="nav-number">3.</span> <span class="nav-text">To be continued…</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">He Guoxiu</span>
</div>



        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->



  



  




	





  




  
  

  

  

  

  


</body>
</html>
